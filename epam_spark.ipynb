{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"zstd\") \\\n",
    "    .config(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sol.shuffle.part-tions\", 200) \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"5\") \\\n",
    "    .config(\"spark.sql.files.maxRecordsPerFile\", 10_000) \\\n",
    "    .config(\"spark.debug.maxToStringFields\", 1000) \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"3600s\") \\\n",
    "    .config(\"spark.network.timeout\", \"7200s\") \\\n",
    "    .config(\"spark.network.timeoutInterval\", \"3600s\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    " \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Import all necessary libraries\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from geopy.geocoders import OpenCage\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "from decimal import Decimal\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkETL\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the restaurants_csv folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "peak memory: 69.37 MiB, increment: 0.13 MiB\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- franchise_id: integer (nullable = true)\n",
      " |-- franchise_name: string (nullable = true)\n",
      " |-- restaurant_franchise_id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      "\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "|          id|franchise_id|      franchise_name|restaurant_franchise_id|country|          city|   lat|    lng|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "|197568495625|          10|    The Golden Spoon|                  24784|     US|       Decatur|34.578|-87.021|\n",
      "| 17179869242|          59|         Azalea Cafe|                  10902|     FR|         Paris|48.861|  2.368|\n",
      "|214748364826|          27|     The Corner Cafe|                  92040|     US|    Rapid City| 44.08|-103.25|\n",
      "|154618822706|          51|        The Pizzeria|                  41484|     AT|        Vienna|48.213| 16.413|\n",
      "|163208757312|          65|       Chef's Corner|                  96638|     GB|        London|51.495| -0.191|\n",
      "| 68719476763|          28|    The Spicy Pickle|                  77517|     US|      Grayling|44.657|-84.744|\n",
      "|223338299419|          28|    The Spicy Pickle|                  36937|     US|        Oswego|43.452|-76.532|\n",
      "|240518168650|          75|     Greenhouse Cafe|                  93164|     NL|     Amsterdam| 52.37|  4.897|\n",
      "|128849018936|          57|The Yellow Submarine|                   5679|     FR|         Paris|48.872|  2.335|\n",
      "|197568495635|          20|       The Brasserie|                  24784|     US|Jeffersonville|39.616|-83.612|\n",
      "| 68719476768|          33|   The Blue Elephant|                  77517|     IT|         Milan|45.479|  9.146|\n",
      "| 51539607582|          31|           Bistro 42|                   6934|     IT|         Milan|45.444|  9.153|\n",
      "| 94489280554|          43|      The Food House|                  95399|     FR|         Paris|48.867|  2.329|\n",
      "|206158430215|           8|     The Green Olive|                  53370|     US|   Haltom City|32.789| -97.28|\n",
      "|154618822657|           2|        Bella Cucina|                  41484|     US|   Fort Pierce|27.412|-80.391|\n",
      "| 17179869217|          34|     The Tasty Treat|                  10902|     US|     Green Bay|44.476|-88.077|\n",
      "|  8589934633|          42|     The Daily Scoop|                  12630|     FR|         Paris|48.854|  2.343|\n",
      "|240518168596|          21|      The Lazy Daisy|                  93164|     US|    Mendenhall| 39.86|-75.646|\n",
      "|171798691906|          67|  Crimson and Clover|                  65939|     NL|     Amsterdam|52.361|  4.894|\n",
      "| 42949673022|          63|          Cafe Paris|                  89646|     GB|        London|51.508| -0.107|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Прежний метод очень доллго грузился по этому использовался для проверки нагрузки\n",
    "%load_ext memory_profiler\n",
    "%memit\n",
    "\n",
    "folder_path = \"restaurant_csv/\"\n",
    "\n",
    "# Получаю список файлов в папке с ресторанами\n",
    "files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# Считайте все CSV-файлы в DataFrame\n",
    "restaurant_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(files)\n",
    "\n",
    "#Cхему DataFrame\n",
    "restaurant_df.printSchema()\n",
    "\n",
    "#DataFrame\n",
    "restaurant_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_key = '8ed618e56d764009b0105f340b74bce5'\n",
    "geolocator = OpenCage(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out records with null latitude or longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out records with null latitude or longitude\n",
    "filtered_df = restaurant_df.filter(restaurant_df[\"lat\"].isNotNull() & restaurant_df[\"lng\"].isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "|          id|franchise_id|      franchise_name|restaurant_franchise_id|country|          city|   lat|    lng|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "|197568495625|          10|    The Golden Spoon|                  24784|     US|       Decatur|34.578|-87.021|\n",
      "| 17179869242|          59|         Azalea Cafe|                  10902|     FR|         Paris|48.861|  2.368|\n",
      "|214748364826|          27|     The Corner Cafe|                  92040|     US|    Rapid City| 44.08|-103.25|\n",
      "|154618822706|          51|        The Pizzeria|                  41484|     AT|        Vienna|48.213| 16.413|\n",
      "|163208757312|          65|       Chef's Corner|                  96638|     GB|        London|51.495| -0.191|\n",
      "| 68719476763|          28|    The Spicy Pickle|                  77517|     US|      Grayling|44.657|-84.744|\n",
      "|223338299419|          28|    The Spicy Pickle|                  36937|     US|        Oswego|43.452|-76.532|\n",
      "|240518168650|          75|     Greenhouse Cafe|                  93164|     NL|     Amsterdam| 52.37|  4.897|\n",
      "|128849018936|          57|The Yellow Submarine|                   5679|     FR|         Paris|48.872|  2.335|\n",
      "|197568495635|          20|       The Brasserie|                  24784|     US|Jeffersonville|39.616|-83.612|\n",
      "| 68719476768|          33|   The Blue Elephant|                  77517|     IT|         Milan|45.479|  9.146|\n",
      "| 51539607582|          31|           Bistro 42|                   6934|     IT|         Milan|45.444|  9.153|\n",
      "| 94489280554|          43|      The Food House|                  95399|     FR|         Paris|48.867|  2.329|\n",
      "|206158430215|           8|     The Green Olive|                  53370|     US|   Haltom City|32.789| -97.28|\n",
      "|154618822657|           2|        Bella Cucina|                  41484|     US|   Fort Pierce|27.412|-80.391|\n",
      "| 17179869217|          34|     The Tasty Treat|                  10902|     US|     Green Bay|44.476|-88.077|\n",
      "|  8589934633|          42|     The Daily Scoop|                  12630|     FR|         Paris|48.854|  2.343|\n",
      "|240518168596|          21|      The Lazy Daisy|                  93164|     US|    Mendenhall| 39.86|-75.646|\n",
      "|171798691906|          67|  Crimson and Clover|                  65939|     NL|     Amsterdam|52.361|  4.894|\n",
      "| 42949673022|          63|          Cafe Paris|                  89646|     GB|        London|51.508| -0.107|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- franchise_id: integer (nullable = true)\n",
      " |-- franchise_name: string (nullable = true)\n",
      " |-- restaurant_franchise_id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cхему DataFrame\n",
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check lat and lng for incorrect values using the OpenCage Geocoding API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1100, in main\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.7, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-81f4f212d403>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Show the resulting DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mresult_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    970\u001b[0m                 )\n\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1323\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1100, in main\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.7, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the reverse_geocode_udf function\n",
    "@udf(StringType())\n",
    "def reverse_geocode_udf(lat, lon):\n",
    "    try:\n",
    "        # Convert 'lat' and 'lon' to float\n",
    "        lat_float = float(lat)\n",
    "        lon_float = float(lon)\n",
    "\n",
    "        # Perform reverse geocoding using OpenCageGeocode API\n",
    "        result = geocoder.reverse_geocode(lat_float, lon_float)\n",
    "\n",
    "        if result and len(result):\n",
    "            return result[0]['formatted']\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        # Handle the case where conversion to float fails\n",
    "        return None\n",
    "\n",
    "# Use withColumn to add a new column 'address'\n",
    "result_df = filtered_df.withColumn(\"address\", reverse_geocode_udf(filtered_df['lat'], filtered_df['lng']))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Im really trying hard but that error doesn't handle\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
